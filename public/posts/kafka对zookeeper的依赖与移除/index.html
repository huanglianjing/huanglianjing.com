<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Kafka对ZooKeeper的依赖与移除 | huanglianjing</title>
<meta name="keywords" content="Kafka, ZooKeeper">
<meta name="description" content="1. 概述 Kafka自2.8开始，移除了之前用于集群的元数据管理、控制器选举等的ZooKeeper的依赖，转而使用Kraft代替，本文来聊聊这一改动的差异和影响。
使用过Kafka都知道，在安装Kafka之前，需要先安装Java和ZooKeeper。需要Java是因为ZooKeeper和Kafka都是用Java编写的，运行需要Java环境。而需要ZooKeeper则是因为，Kafka是使用ZooKeeper来保存集群的元数据信息和消费者信息，进行控制器选举的，因此Kafka的运行需要ZooKeeper的支持。
而在Kafka 2.8中，将移除对ZooKeeper的依赖，转而使用基于KRaft的Quorum控制器。
但是目前据官方声称有些功能还不是太完善，建议先不要用于生产环境。
2. 依赖ZooKeeper的Kafka Kafka体系架构包含若干Producer、若干Broker、若干Consumer，以及一个ZooKeeper集群。Kafka通过ZooKeeper管理集群broker和消费者的元数据，以及用于进行控制器选举。
3. Kafka在ZooKeeper中的数据 3.1 选举控制器 Kafka通过ZooKeeper的临时节点实现选举控制器的功能。
Broker集群中，第一个启动的broker会在ZooKeeper中创建临时节点/controller，从而成为控制器。其他broker启动后也会尝试创建该临时节点，但是会收到节点已存在的异常，从而获知控制器节点已存在，并注册一个监听器。
在控制器关闭或断开连接后，临时节点/controller被删除，其他broker通过监听得到通知并再次尝试创建临时节点/controller，第一个创建成功的broker成为新的控制器，其他获得异常并再次进行监听。
3.2 Broker注册 Kafka的Broker是分布式部署并且相互间独立运行的，但是需要在ZooKeeper进行注册，以将整个集群的Broker服务器管理起来。
ZooKeeper中专门使用Broker节点进行Broker服务器列表的记录，节点路径为/brokers/ids。
每个Broker服务器在启动时，都会到ZooKeeper上进行注册，根据各自的Broker ID创建属于自己的节点，节点路径为/brokers/ids/[0&hellip;N]，并且将自己的IP地址和端口等信息写入该节点。
这个节点是一个临时节点，在Broker服务器宕机或是下线后，对应的节点就被删除了。
3.3 Topic注册 在Kafka中，一个Topic的消息会分成多个分区并分布到多个Broker上，这些分区信息以及与Broker的对应关系也都是ZooKeeper维护的，使用专门的Topic节点来记录，节点路径为/brokers/topics。
每一个Kafka的Topic都记录在/brokers/topics/[topic]节点中。
Broker服务器在启动后，会到对应的Topic节点下注册自己的Broker ID，并写入针对该Topic的提供消息存储的分区总数。如节点路径/brokers/topics/[topic]/[broker_id]的节点内容为2，表明该broker在该topic中提供了2个分区的消息存储。
这个节点也是一个临时节点，在Broker服务器宕机或是下线后，对应的节点就被删除了。
3.4 消费者注册 每个消费者在启动的时候，都会在ZooKeeper创建一个属于自己的消费者节点，且为临时节点，这样消费者出现故障或是下线后，对应的消费者节点就会被删除掉。
节点路径为/consumers/[group_id]/ids/[consumer_id]，节点内容为消费者订阅的Topic信息。
每个消费者都会对所属消费者分组的/consumers/[group_id]/ids节点进行Watcher监听，以在消费者新增或减少时，触发消费者负载均衡。同时对/brokers/ids/[0&hellip;N]的节点注册监听，在broker服务器列表发生变化时，根据情况决定是否需要消费者的负载均衡。
3.5 消息分区与消费者关系 对于每个消费者分组，Kafka都会为其分配一个全局唯一的Group ID，同时Kafka也会为每个消费者分配一个Consumer ID，通常为Hostname:UUID的形式。
Kafka的设计规定了每个消息分区有且只有一个消费者进行消息消费，因此需要在ZooKeeper记录消息分区与消费者之间的对应关系。节点路径/consumers/[group_id]/owners/[topic]/[broker_id-partition_id]，节点内容为Consumer ID，记录了一个消费者分组对于各个主题的分区所分配的消费者id。
3.6 消息消费进度Offset记录 ZooKeeper同时还记录了消费者对指定消息分区进行消息消费的过程中，定时提交的消费进度，即Offset。这有助于在该消费者重启，或其他消费者接管该消息分区的消息消费后，能够从之前的进度开始继续消息的消费。
节点路径为/consumers/[group_id]/offsets/[topic]/[broker_id-partition_id]，节点内容就是Offset值。
4. 为什么要去掉ZooKeeper 在之前的版本中，Kafka依赖于ZooKeeper，需要管理部署两个不同的系统，让运维复杂度翻倍，还让Kafka变得沉重，进而限制了Kafka在轻量环境下的应用。去掉ZooKeeper使得Kafka的部署更简单，更轻量级。 ZooKeeper的分区特性限制Kafka的承载能力。在分区数量多的时候，控制器节点重新选举、分区首领切换需要进行很多的ZooKeeper操作，ZooKeeper存储的元数据数据量也更多，可能导致监听的延时增长或丢失，故障恢复耗时也更长。 Raft协议比ZooKeeper的ZAB协议更易懂，更高效，能提高控制器选举的速度和分区首领选举的速度。 5. 去掉ZooKeeper的Kafka 5.1 体系架构对比 左图所示为目前的结构，有着3个ZooKeeper节点和4个Kafka broker节点，其中橙色的为broker节点中的控制器，控制器会向其他broker节点推送消息。
右图所示为去掉ZooKeeper后的结构，用控制器节点代替了ZooKeeper节点，还有普通的broker节点。控制器节点为元数据分区选举出一个橙色的leader，也称为活动控制器（active controller），其他控制器节点为follower节点。普通节点从leader拉取元数据。控制器进程与broker进程逻辑上分开，但是物理上不必分开，可以部署在同样的节点上。
元数据的更新也从通过向ZooKeeper注册监听的方式修改为普通节点主动从活动控制器拉取的方式。
5.2 quorum控制器 新的架构使用quorum控制器来事件来管理元数据日志，元数据日志包含集群元数据的每次更改，之前存在ZooKeeper的主题、分区、配置等信息都存在这个日志中。控制器节点中leader节点（上方右图的橙色节点）处理所有broker节点的请求，而其他follower节点（上方右图的蓝色节点节点）则复制数据，以在leader节点故障时转移状态为leader。Raft需要超过半数节点运行才能继续运行，所以三个节点的控制器允许一个节点故障，五个节点的控制器允许两个节点的故障。
元数据的更新也从通过向ZooKeeper注册监听的方式修改为普通节点主动从活动控制器拉取的方式。
5.3 启动方式 为Kafka集群生成一个集群ID
$ ./bin/kafka-storage.sh random-uuid xtzWWN4bTjitpL3kfd9s5g 用生成的ID格式化存储目录，对于单节点在该节点运行，对多节点则在每一个节点运行">
<meta name="author" content="">
<link rel="canonical" href="http://huanglianjing.com/posts/kafka%E5%AF%B9zookeeper%E7%9A%84%E4%BE%9D%E8%B5%96%E4%B8%8E%E7%A7%BB%E9%99%A4/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css" integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe&#43;FVUFzPh7U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="http://huanglianjing.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://huanglianjing.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://huanglianjing.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://huanglianjing.com/apple-touch-icon.png">
<link rel="mask-icon" href="http://huanglianjing.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Kafka对ZooKeeper的依赖与移除" />
<meta property="og:description" content="1. 概述 Kafka自2.8开始，移除了之前用于集群的元数据管理、控制器选举等的ZooKeeper的依赖，转而使用Kraft代替，本文来聊聊这一改动的差异和影响。
使用过Kafka都知道，在安装Kafka之前，需要先安装Java和ZooKeeper。需要Java是因为ZooKeeper和Kafka都是用Java编写的，运行需要Java环境。而需要ZooKeeper则是因为，Kafka是使用ZooKeeper来保存集群的元数据信息和消费者信息，进行控制器选举的，因此Kafka的运行需要ZooKeeper的支持。
而在Kafka 2.8中，将移除对ZooKeeper的依赖，转而使用基于KRaft的Quorum控制器。
但是目前据官方声称有些功能还不是太完善，建议先不要用于生产环境。
2. 依赖ZooKeeper的Kafka Kafka体系架构包含若干Producer、若干Broker、若干Consumer，以及一个ZooKeeper集群。Kafka通过ZooKeeper管理集群broker和消费者的元数据，以及用于进行控制器选举。
3. Kafka在ZooKeeper中的数据 3.1 选举控制器 Kafka通过ZooKeeper的临时节点实现选举控制器的功能。
Broker集群中，第一个启动的broker会在ZooKeeper中创建临时节点/controller，从而成为控制器。其他broker启动后也会尝试创建该临时节点，但是会收到节点已存在的异常，从而获知控制器节点已存在，并注册一个监听器。
在控制器关闭或断开连接后，临时节点/controller被删除，其他broker通过监听得到通知并再次尝试创建临时节点/controller，第一个创建成功的broker成为新的控制器，其他获得异常并再次进行监听。
3.2 Broker注册 Kafka的Broker是分布式部署并且相互间独立运行的，但是需要在ZooKeeper进行注册，以将整个集群的Broker服务器管理起来。
ZooKeeper中专门使用Broker节点进行Broker服务器列表的记录，节点路径为/brokers/ids。
每个Broker服务器在启动时，都会到ZooKeeper上进行注册，根据各自的Broker ID创建属于自己的节点，节点路径为/brokers/ids/[0&hellip;N]，并且将自己的IP地址和端口等信息写入该节点。
这个节点是一个临时节点，在Broker服务器宕机或是下线后，对应的节点就被删除了。
3.3 Topic注册 在Kafka中，一个Topic的消息会分成多个分区并分布到多个Broker上，这些分区信息以及与Broker的对应关系也都是ZooKeeper维护的，使用专门的Topic节点来记录，节点路径为/brokers/topics。
每一个Kafka的Topic都记录在/brokers/topics/[topic]节点中。
Broker服务器在启动后，会到对应的Topic节点下注册自己的Broker ID，并写入针对该Topic的提供消息存储的分区总数。如节点路径/brokers/topics/[topic]/[broker_id]的节点内容为2，表明该broker在该topic中提供了2个分区的消息存储。
这个节点也是一个临时节点，在Broker服务器宕机或是下线后，对应的节点就被删除了。
3.4 消费者注册 每个消费者在启动的时候，都会在ZooKeeper创建一个属于自己的消费者节点，且为临时节点，这样消费者出现故障或是下线后，对应的消费者节点就会被删除掉。
节点路径为/consumers/[group_id]/ids/[consumer_id]，节点内容为消费者订阅的Topic信息。
每个消费者都会对所属消费者分组的/consumers/[group_id]/ids节点进行Watcher监听，以在消费者新增或减少时，触发消费者负载均衡。同时对/brokers/ids/[0&hellip;N]的节点注册监听，在broker服务器列表发生变化时，根据情况决定是否需要消费者的负载均衡。
3.5 消息分区与消费者关系 对于每个消费者分组，Kafka都会为其分配一个全局唯一的Group ID，同时Kafka也会为每个消费者分配一个Consumer ID，通常为Hostname:UUID的形式。
Kafka的设计规定了每个消息分区有且只有一个消费者进行消息消费，因此需要在ZooKeeper记录消息分区与消费者之间的对应关系。节点路径/consumers/[group_id]/owners/[topic]/[broker_id-partition_id]，节点内容为Consumer ID，记录了一个消费者分组对于各个主题的分区所分配的消费者id。
3.6 消息消费进度Offset记录 ZooKeeper同时还记录了消费者对指定消息分区进行消息消费的过程中，定时提交的消费进度，即Offset。这有助于在该消费者重启，或其他消费者接管该消息分区的消息消费后，能够从之前的进度开始继续消息的消费。
节点路径为/consumers/[group_id]/offsets/[topic]/[broker_id-partition_id]，节点内容就是Offset值。
4. 为什么要去掉ZooKeeper 在之前的版本中，Kafka依赖于ZooKeeper，需要管理部署两个不同的系统，让运维复杂度翻倍，还让Kafka变得沉重，进而限制了Kafka在轻量环境下的应用。去掉ZooKeeper使得Kafka的部署更简单，更轻量级。 ZooKeeper的分区特性限制Kafka的承载能力。在分区数量多的时候，控制器节点重新选举、分区首领切换需要进行很多的ZooKeeper操作，ZooKeeper存储的元数据数据量也更多，可能导致监听的延时增长或丢失，故障恢复耗时也更长。 Raft协议比ZooKeeper的ZAB协议更易懂，更高效，能提高控制器选举的速度和分区首领选举的速度。 5. 去掉ZooKeeper的Kafka 5.1 体系架构对比 左图所示为目前的结构，有着3个ZooKeeper节点和4个Kafka broker节点，其中橙色的为broker节点中的控制器，控制器会向其他broker节点推送消息。
右图所示为去掉ZooKeeper后的结构，用控制器节点代替了ZooKeeper节点，还有普通的broker节点。控制器节点为元数据分区选举出一个橙色的leader，也称为活动控制器（active controller），其他控制器节点为follower节点。普通节点从leader拉取元数据。控制器进程与broker进程逻辑上分开，但是物理上不必分开，可以部署在同样的节点上。
元数据的更新也从通过向ZooKeeper注册监听的方式修改为普通节点主动从活动控制器拉取的方式。
5.2 quorum控制器 新的架构使用quorum控制器来事件来管理元数据日志，元数据日志包含集群元数据的每次更改，之前存在ZooKeeper的主题、分区、配置等信息都存在这个日志中。控制器节点中leader节点（上方右图的橙色节点）处理所有broker节点的请求，而其他follower节点（上方右图的蓝色节点节点）则复制数据，以在leader节点故障时转移状态为leader。Raft需要超过半数节点运行才能继续运行，所以三个节点的控制器允许一个节点故障，五个节点的控制器允许两个节点的故障。
元数据的更新也从通过向ZooKeeper注册监听的方式修改为普通节点主动从活动控制器拉取的方式。
5.3 启动方式 为Kafka集群生成一个集群ID
$ ./bin/kafka-storage.sh random-uuid xtzWWN4bTjitpL3kfd9s5g 用生成的ID格式化存储目录，对于单节点在该节点运行，对多节点则在每一个节点运行" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://huanglianjing.com/posts/kafka%E5%AF%B9zookeeper%E7%9A%84%E4%BE%9D%E8%B5%96%E4%B8%8E%E7%A7%BB%E9%99%A4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-25T02:05:51+08:00" />
<meta property="article:modified_time" content="2023-07-25T02:05:51+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Kafka对ZooKeeper的依赖与移除"/>
<meta name="twitter:description" content="1. 概述 Kafka自2.8开始，移除了之前用于集群的元数据管理、控制器选举等的ZooKeeper的依赖，转而使用Kraft代替，本文来聊聊这一改动的差异和影响。
使用过Kafka都知道，在安装Kafka之前，需要先安装Java和ZooKeeper。需要Java是因为ZooKeeper和Kafka都是用Java编写的，运行需要Java环境。而需要ZooKeeper则是因为，Kafka是使用ZooKeeper来保存集群的元数据信息和消费者信息，进行控制器选举的，因此Kafka的运行需要ZooKeeper的支持。
而在Kafka 2.8中，将移除对ZooKeeper的依赖，转而使用基于KRaft的Quorum控制器。
但是目前据官方声称有些功能还不是太完善，建议先不要用于生产环境。
2. 依赖ZooKeeper的Kafka Kafka体系架构包含若干Producer、若干Broker、若干Consumer，以及一个ZooKeeper集群。Kafka通过ZooKeeper管理集群broker和消费者的元数据，以及用于进行控制器选举。
3. Kafka在ZooKeeper中的数据 3.1 选举控制器 Kafka通过ZooKeeper的临时节点实现选举控制器的功能。
Broker集群中，第一个启动的broker会在ZooKeeper中创建临时节点/controller，从而成为控制器。其他broker启动后也会尝试创建该临时节点，但是会收到节点已存在的异常，从而获知控制器节点已存在，并注册一个监听器。
在控制器关闭或断开连接后，临时节点/controller被删除，其他broker通过监听得到通知并再次尝试创建临时节点/controller，第一个创建成功的broker成为新的控制器，其他获得异常并再次进行监听。
3.2 Broker注册 Kafka的Broker是分布式部署并且相互间独立运行的，但是需要在ZooKeeper进行注册，以将整个集群的Broker服务器管理起来。
ZooKeeper中专门使用Broker节点进行Broker服务器列表的记录，节点路径为/brokers/ids。
每个Broker服务器在启动时，都会到ZooKeeper上进行注册，根据各自的Broker ID创建属于自己的节点，节点路径为/brokers/ids/[0&hellip;N]，并且将自己的IP地址和端口等信息写入该节点。
这个节点是一个临时节点，在Broker服务器宕机或是下线后，对应的节点就被删除了。
3.3 Topic注册 在Kafka中，一个Topic的消息会分成多个分区并分布到多个Broker上，这些分区信息以及与Broker的对应关系也都是ZooKeeper维护的，使用专门的Topic节点来记录，节点路径为/brokers/topics。
每一个Kafka的Topic都记录在/brokers/topics/[topic]节点中。
Broker服务器在启动后，会到对应的Topic节点下注册自己的Broker ID，并写入针对该Topic的提供消息存储的分区总数。如节点路径/brokers/topics/[topic]/[broker_id]的节点内容为2，表明该broker在该topic中提供了2个分区的消息存储。
这个节点也是一个临时节点，在Broker服务器宕机或是下线后，对应的节点就被删除了。
3.4 消费者注册 每个消费者在启动的时候，都会在ZooKeeper创建一个属于自己的消费者节点，且为临时节点，这样消费者出现故障或是下线后，对应的消费者节点就会被删除掉。
节点路径为/consumers/[group_id]/ids/[consumer_id]，节点内容为消费者订阅的Topic信息。
每个消费者都会对所属消费者分组的/consumers/[group_id]/ids节点进行Watcher监听，以在消费者新增或减少时，触发消费者负载均衡。同时对/brokers/ids/[0&hellip;N]的节点注册监听，在broker服务器列表发生变化时，根据情况决定是否需要消费者的负载均衡。
3.5 消息分区与消费者关系 对于每个消费者分组，Kafka都会为其分配一个全局唯一的Group ID，同时Kafka也会为每个消费者分配一个Consumer ID，通常为Hostname:UUID的形式。
Kafka的设计规定了每个消息分区有且只有一个消费者进行消息消费，因此需要在ZooKeeper记录消息分区与消费者之间的对应关系。节点路径/consumers/[group_id]/owners/[topic]/[broker_id-partition_id]，节点内容为Consumer ID，记录了一个消费者分组对于各个主题的分区所分配的消费者id。
3.6 消息消费进度Offset记录 ZooKeeper同时还记录了消费者对指定消息分区进行消息消费的过程中，定时提交的消费进度，即Offset。这有助于在该消费者重启，或其他消费者接管该消息分区的消息消费后，能够从之前的进度开始继续消息的消费。
节点路径为/consumers/[group_id]/offsets/[topic]/[broker_id-partition_id]，节点内容就是Offset值。
4. 为什么要去掉ZooKeeper 在之前的版本中，Kafka依赖于ZooKeeper，需要管理部署两个不同的系统，让运维复杂度翻倍，还让Kafka变得沉重，进而限制了Kafka在轻量环境下的应用。去掉ZooKeeper使得Kafka的部署更简单，更轻量级。 ZooKeeper的分区特性限制Kafka的承载能力。在分区数量多的时候，控制器节点重新选举、分区首领切换需要进行很多的ZooKeeper操作，ZooKeeper存储的元数据数据量也更多，可能导致监听的延时增长或丢失，故障恢复耗时也更长。 Raft协议比ZooKeeper的ZAB协议更易懂，更高效，能提高控制器选举的速度和分区首领选举的速度。 5. 去掉ZooKeeper的Kafka 5.1 体系架构对比 左图所示为目前的结构，有着3个ZooKeeper节点和4个Kafka broker节点，其中橙色的为broker节点中的控制器，控制器会向其他broker节点推送消息。
右图所示为去掉ZooKeeper后的结构，用控制器节点代替了ZooKeeper节点，还有普通的broker节点。控制器节点为元数据分区选举出一个橙色的leader，也称为活动控制器（active controller），其他控制器节点为follower节点。普通节点从leader拉取元数据。控制器进程与broker进程逻辑上分开，但是物理上不必分开，可以部署在同样的节点上。
元数据的更新也从通过向ZooKeeper注册监听的方式修改为普通节点主动从活动控制器拉取的方式。
5.2 quorum控制器 新的架构使用quorum控制器来事件来管理元数据日志，元数据日志包含集群元数据的每次更改，之前存在ZooKeeper的主题、分区、配置等信息都存在这个日志中。控制器节点中leader节点（上方右图的橙色节点）处理所有broker节点的请求，而其他follower节点（上方右图的蓝色节点节点）则复制数据，以在leader节点故障时转移状态为leader。Raft需要超过半数节点运行才能继续运行，所以三个节点的控制器允许一个节点故障，五个节点的控制器允许两个节点的故障。
元数据的更新也从通过向ZooKeeper注册监听的方式修改为普通节点主动从活动控制器拉取的方式。
5.3 启动方式 为Kafka集群生成一个集群ID
$ ./bin/kafka-storage.sh random-uuid xtzWWN4bTjitpL3kfd9s5g 用生成的ID格式化存储目录，对于单节点在该节点运行，对多节点则在每一个节点运行"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://huanglianjing.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Kafka对ZooKeeper的依赖与移除",
      "item": "http://huanglianjing.com/posts/kafka%E5%AF%B9zookeeper%E7%9A%84%E4%BE%9D%E8%B5%96%E4%B8%8E%E7%A7%BB%E9%99%A4/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Kafka对ZooKeeper的依赖与移除",
  "name": "Kafka对ZooKeeper的依赖与移除",
  "description": "1. 概述 Kafka自2.8开始，移除了之前用于集群的元数据管理、控制器选举等的ZooKeeper的依赖，转而使用Kraft代替，本文来聊聊这一改动的差异和影响。\n使用过Kafka都知道，在安装Kafka之前，需要先安装Java和ZooKeeper。需要Java是因为ZooKeeper和Kafka都是用Java编写的，运行需要Java环境。而需要ZooKeeper则是因为，Kafka是使用ZooKeeper来保存集群的元数据信息和消费者信息，进行控制器选举的，因此Kafka的运行需要ZooKeeper的支持。\n而在Kafka 2.8中，将移除对ZooKeeper的依赖，转而使用基于KRaft的Quorum控制器。\n但是目前据官方声称有些功能还不是太完善，建议先不要用于生产环境。\n2. 依赖ZooKeeper的Kafka Kafka体系架构包含若干Producer、若干Broker、若干Consumer，以及一个ZooKeeper集群。Kafka通过ZooKeeper管理集群broker和消费者的元数据，以及用于进行控制器选举。\n3. Kafka在ZooKeeper中的数据 3.1 选举控制器 Kafka通过ZooKeeper的临时节点实现选举控制器的功能。\nBroker集群中，第一个启动的broker会在ZooKeeper中创建临时节点/controller，从而成为控制器。其他broker启动后也会尝试创建该临时节点，但是会收到节点已存在的异常，从而获知控制器节点已存在，并注册一个监听器。\n在控制器关闭或断开连接后，临时节点/controller被删除，其他broker通过监听得到通知并再次尝试创建临时节点/controller，第一个创建成功的broker成为新的控制器，其他获得异常并再次进行监听。\n3.2 Broker注册 Kafka的Broker是分布式部署并且相互间独立运行的，但是需要在ZooKeeper进行注册，以将整个集群的Broker服务器管理起来。\nZooKeeper中专门使用Broker节点进行Broker服务器列表的记录，节点路径为/brokers/ids。\n每个Broker服务器在启动时，都会到ZooKeeper上进行注册，根据各自的Broker ID创建属于自己的节点，节点路径为/brokers/ids/[0\u0026hellip;N]，并且将自己的IP地址和端口等信息写入该节点。\n这个节点是一个临时节点，在Broker服务器宕机或是下线后，对应的节点就被删除了。\n3.3 Topic注册 在Kafka中，一个Topic的消息会分成多个分区并分布到多个Broker上，这些分区信息以及与Broker的对应关系也都是ZooKeeper维护的，使用专门的Topic节点来记录，节点路径为/brokers/topics。\n每一个Kafka的Topic都记录在/brokers/topics/[topic]节点中。\nBroker服务器在启动后，会到对应的Topic节点下注册自己的Broker ID，并写入针对该Topic的提供消息存储的分区总数。如节点路径/brokers/topics/[topic]/[broker_id]的节点内容为2，表明该broker在该topic中提供了2个分区的消息存储。\n这个节点也是一个临时节点，在Broker服务器宕机或是下线后，对应的节点就被删除了。\n3.4 消费者注册 每个消费者在启动的时候，都会在ZooKeeper创建一个属于自己的消费者节点，且为临时节点，这样消费者出现故障或是下线后，对应的消费者节点就会被删除掉。\n节点路径为/consumers/[group_id]/ids/[consumer_id]，节点内容为消费者订阅的Topic信息。\n每个消费者都会对所属消费者分组的/consumers/[group_id]/ids节点进行Watcher监听，以在消费者新增或减少时，触发消费者负载均衡。同时对/brokers/ids/[0\u0026hellip;N]的节点注册监听，在broker服务器列表发生变化时，根据情况决定是否需要消费者的负载均衡。\n3.5 消息分区与消费者关系 对于每个消费者分组，Kafka都会为其分配一个全局唯一的Group ID，同时Kafka也会为每个消费者分配一个Consumer ID，通常为Hostname:UUID的形式。\nKafka的设计规定了每个消息分区有且只有一个消费者进行消息消费，因此需要在ZooKeeper记录消息分区与消费者之间的对应关系。节点路径/consumers/[group_id]/owners/[topic]/[broker_id-partition_id]，节点内容为Consumer ID，记录了一个消费者分组对于各个主题的分区所分配的消费者id。\n3.6 消息消费进度Offset记录 ZooKeeper同时还记录了消费者对指定消息分区进行消息消费的过程中，定时提交的消费进度，即Offset。这有助于在该消费者重启，或其他消费者接管该消息分区的消息消费后，能够从之前的进度开始继续消息的消费。\n节点路径为/consumers/[group_id]/offsets/[topic]/[broker_id-partition_id]，节点内容就是Offset值。\n4. 为什么要去掉ZooKeeper 在之前的版本中，Kafka依赖于ZooKeeper，需要管理部署两个不同的系统，让运维复杂度翻倍，还让Kafka变得沉重，进而限制了Kafka在轻量环境下的应用。去掉ZooKeeper使得Kafka的部署更简单，更轻量级。 ZooKeeper的分区特性限制Kafka的承载能力。在分区数量多的时候，控制器节点重新选举、分区首领切换需要进行很多的ZooKeeper操作，ZooKeeper存储的元数据数据量也更多，可能导致监听的延时增长或丢失，故障恢复耗时也更长。 Raft协议比ZooKeeper的ZAB协议更易懂，更高效，能提高控制器选举的速度和分区首领选举的速度。 5. 去掉ZooKeeper的Kafka 5.1 体系架构对比 左图所示为目前的结构，有着3个ZooKeeper节点和4个Kafka broker节点，其中橙色的为broker节点中的控制器，控制器会向其他broker节点推送消息。\n右图所示为去掉ZooKeeper后的结构，用控制器节点代替了ZooKeeper节点，还有普通的broker节点。控制器节点为元数据分区选举出一个橙色的leader，也称为活动控制器（active controller），其他控制器节点为follower节点。普通节点从leader拉取元数据。控制器进程与broker进程逻辑上分开，但是物理上不必分开，可以部署在同样的节点上。\n元数据的更新也从通过向ZooKeeper注册监听的方式修改为普通节点主动从活动控制器拉取的方式。\n5.2 quorum控制器 新的架构使用quorum控制器来事件来管理元数据日志，元数据日志包含集群元数据的每次更改，之前存在ZooKeeper的主题、分区、配置等信息都存在这个日志中。控制器节点中leader节点（上方右图的橙色节点）处理所有broker节点的请求，而其他follower节点（上方右图的蓝色节点节点）则复制数据，以在leader节点故障时转移状态为leader。Raft需要超过半数节点运行才能继续运行，所以三个节点的控制器允许一个节点故障，五个节点的控制器允许两个节点的故障。\n元数据的更新也从通过向ZooKeeper注册监听的方式修改为普通节点主动从活动控制器拉取的方式。\n5.3 启动方式 为Kafka集群生成一个集群ID\n$ ./bin/kafka-storage.sh random-uuid xtzWWN4bTjitpL3kfd9s5g 用生成的ID格式化存储目录，对于单节点在该节点运行，对多节点则在每一个节点运行",
  "keywords": [
    "Kafka", "ZooKeeper"
  ],
  "articleBody": "1. 概述 Kafka自2.8开始，移除了之前用于集群的元数据管理、控制器选举等的ZooKeeper的依赖，转而使用Kraft代替，本文来聊聊这一改动的差异和影响。\n使用过Kafka都知道，在安装Kafka之前，需要先安装Java和ZooKeeper。需要Java是因为ZooKeeper和Kafka都是用Java编写的，运行需要Java环境。而需要ZooKeeper则是因为，Kafka是使用ZooKeeper来保存集群的元数据信息和消费者信息，进行控制器选举的，因此Kafka的运行需要ZooKeeper的支持。\n而在Kafka 2.8中，将移除对ZooKeeper的依赖，转而使用基于KRaft的Quorum控制器。\n但是目前据官方声称有些功能还不是太完善，建议先不要用于生产环境。\n2. 依赖ZooKeeper的Kafka Kafka体系架构包含若干Producer、若干Broker、若干Consumer，以及一个ZooKeeper集群。Kafka通过ZooKeeper管理集群broker和消费者的元数据，以及用于进行控制器选举。\n3. Kafka在ZooKeeper中的数据 3.1 选举控制器 Kafka通过ZooKeeper的临时节点实现选举控制器的功能。\nBroker集群中，第一个启动的broker会在ZooKeeper中创建临时节点/controller，从而成为控制器。其他broker启动后也会尝试创建该临时节点，但是会收到节点已存在的异常，从而获知控制器节点已存在，并注册一个监听器。\n在控制器关闭或断开连接后，临时节点/controller被删除，其他broker通过监听得到通知并再次尝试创建临时节点/controller，第一个创建成功的broker成为新的控制器，其他获得异常并再次进行监听。\n3.2 Broker注册 Kafka的Broker是分布式部署并且相互间独立运行的，但是需要在ZooKeeper进行注册，以将整个集群的Broker服务器管理起来。\nZooKeeper中专门使用Broker节点进行Broker服务器列表的记录，节点路径为/brokers/ids。\n每个Broker服务器在启动时，都会到ZooKeeper上进行注册，根据各自的Broker ID创建属于自己的节点，节点路径为/brokers/ids/[0…N]，并且将自己的IP地址和端口等信息写入该节点。\n这个节点是一个临时节点，在Broker服务器宕机或是下线后，对应的节点就被删除了。\n3.3 Topic注册 在Kafka中，一个Topic的消息会分成多个分区并分布到多个Broker上，这些分区信息以及与Broker的对应关系也都是ZooKeeper维护的，使用专门的Topic节点来记录，节点路径为/brokers/topics。\n每一个Kafka的Topic都记录在/brokers/topics/[topic]节点中。\nBroker服务器在启动后，会到对应的Topic节点下注册自己的Broker ID，并写入针对该Topic的提供消息存储的分区总数。如节点路径/brokers/topics/[topic]/[broker_id]的节点内容为2，表明该broker在该topic中提供了2个分区的消息存储。\n这个节点也是一个临时节点，在Broker服务器宕机或是下线后，对应的节点就被删除了。\n3.4 消费者注册 每个消费者在启动的时候，都会在ZooKeeper创建一个属于自己的消费者节点，且为临时节点，这样消费者出现故障或是下线后，对应的消费者节点就会被删除掉。\n节点路径为/consumers/[group_id]/ids/[consumer_id]，节点内容为消费者订阅的Topic信息。\n每个消费者都会对所属消费者分组的/consumers/[group_id]/ids节点进行Watcher监听，以在消费者新增或减少时，触发消费者负载均衡。同时对/brokers/ids/[0…N]的节点注册监听，在broker服务器列表发生变化时，根据情况决定是否需要消费者的负载均衡。\n3.5 消息分区与消费者关系 对于每个消费者分组，Kafka都会为其分配一个全局唯一的Group ID，同时Kafka也会为每个消费者分配一个Consumer ID，通常为Hostname:UUID的形式。\nKafka的设计规定了每个消息分区有且只有一个消费者进行消息消费，因此需要在ZooKeeper记录消息分区与消费者之间的对应关系。节点路径/consumers/[group_id]/owners/[topic]/[broker_id-partition_id]，节点内容为Consumer ID，记录了一个消费者分组对于各个主题的分区所分配的消费者id。\n3.6 消息消费进度Offset记录 ZooKeeper同时还记录了消费者对指定消息分区进行消息消费的过程中，定时提交的消费进度，即Offset。这有助于在该消费者重启，或其他消费者接管该消息分区的消息消费后，能够从之前的进度开始继续消息的消费。\n节点路径为/consumers/[group_id]/offsets/[topic]/[broker_id-partition_id]，节点内容就是Offset值。\n4. 为什么要去掉ZooKeeper 在之前的版本中，Kafka依赖于ZooKeeper，需要管理部署两个不同的系统，让运维复杂度翻倍，还让Kafka变得沉重，进而限制了Kafka在轻量环境下的应用。去掉ZooKeeper使得Kafka的部署更简单，更轻量级。 ZooKeeper的分区特性限制Kafka的承载能力。在分区数量多的时候，控制器节点重新选举、分区首领切换需要进行很多的ZooKeeper操作，ZooKeeper存储的元数据数据量也更多，可能导致监听的延时增长或丢失，故障恢复耗时也更长。 Raft协议比ZooKeeper的ZAB协议更易懂，更高效，能提高控制器选举的速度和分区首领选举的速度。 5. 去掉ZooKeeper的Kafka 5.1 体系架构对比 左图所示为目前的结构，有着3个ZooKeeper节点和4个Kafka broker节点，其中橙色的为broker节点中的控制器，控制器会向其他broker节点推送消息。\n右图所示为去掉ZooKeeper后的结构，用控制器节点代替了ZooKeeper节点，还有普通的broker节点。控制器节点为元数据分区选举出一个橙色的leader，也称为活动控制器（active controller），其他控制器节点为follower节点。普通节点从leader拉取元数据。控制器进程与broker进程逻辑上分开，但是物理上不必分开，可以部署在同样的节点上。\n元数据的更新也从通过向ZooKeeper注册监听的方式修改为普通节点主动从活动控制器拉取的方式。\n5.2 quorum控制器 新的架构使用quorum控制器来事件来管理元数据日志，元数据日志包含集群元数据的每次更改，之前存在ZooKeeper的主题、分区、配置等信息都存在这个日志中。控制器节点中leader节点（上方右图的橙色节点）处理所有broker节点的请求，而其他follower节点（上方右图的蓝色节点节点）则复制数据，以在leader节点故障时转移状态为leader。Raft需要超过半数节点运行才能继续运行，所以三个节点的控制器允许一个节点故障，五个节点的控制器允许两个节点的故障。\n元数据的更新也从通过向ZooKeeper注册监听的方式修改为普通节点主动从活动控制器拉取的方式。\n5.3 启动方式 为Kafka集群生成一个集群ID\n$ ./bin/kafka-storage.sh random-uuid xtzWWN4bTjitpL3kfd9s5g 用生成的ID格式化存储目录，对于单节点在该节点运行，对多节点则在每一个节点运行\n$ ./bin/kafka-storage.sh format -t -c ./config/kraft/server.properties Formatting /tmp/kraft-combined-logs 在每个节点上启动Kafka，然后Kafka就启动起来了，过程中没有用到ZooKeeper\n$ ./bin/kafka-server-start.sh ./config/kraft/server.properties 5.4 配置 每个Kafka broker需要在配置文件中配置一个process.roles，他的值可能是如下几个：\nbroker，该节点设置为KRaft模式的一个普通节点 controller，该节点设置为KRaft模式的控制器节点 broker,controller，该节点设置为KRaft模式的既是控制器节点也是普通节点，上面说了，控制器节点和普通节点在逻辑上不同，但是是可以部署在同一个节点上的 未设置，该节点设置为ZooKeeper模式 每个控制器节点需要配置节点id，即node.id，例如集群中配置3个控制器节点，节点id可以设置为0,1,2\nnode.id=0 每个控制器节点和普通节点必须配置监听地址\n# 普通节点 listeners=PLAINTEXT://localhost:9092 # 控制器节点 listeners=PLAINTEXT://:9093 # 普通节点\u0026控制器节点 listeners=PLAINTEXT://:9092,CONTROLLER://:9093 每个控制器节点和普通节点必须配置Quorum Voters，也就是controller.quorum.voters\ncontroller.quorum.voters=id1@host1:port1,id2@host2:port2,id3@host3:port3 元数据日志查看的两种方式\n# kafka-dump-log $ ./bin/kafka-dump-log.sh --cluster-metadata-decoder --skip-record-metadata --files /tmp/kraft-combined-logs/\\@metadata-0/*.log # Metadata Shell $ ./bin/kafka-metadata-shell.sh --snapshot /tmp/kraft-combined-logs/\\@metadata-0/00000000000000000000.log \u003e\u003e ls / brokers local metadataQuorum topicIds topics \u003e\u003e ls /topics foo \u003e\u003e cat /topics/foo/0/data { \"partitionId\" : 0, \"topicId\" : \"5zoAlv-xEh9xRANKXt1Lbg\", \"replicas\" : [ 1 ], \"isr\" : [ 1 ], \"removingReplicas\" : null, \"addingReplicas\" : null, \"leader\" : 1, \"leaderEpoch\" : 0, \"partitionEpoch\" : 0 } \u003e\u003e exit 5.5 Raft算法 以下动画网页很好地展示了Raft算法的原理和同步、复制、领导选举等步骤：\nhttp://thesecretlivesofdata.com/raft/\n5.6 性能对比 quorum控制器的引入极大降低了多分区情形下关闭与重启的耗时，从而提高了分区的数量上限。\n以下为Kafka在新旧的架构中拥有200万分区时关闭和启动恢复的速度，可以看出在新的架构下时间是大大缩短的。\n参考 《从Paxos到Zookeeper》 《Kafka权威指南》 KIP-500: Replace ZooKeeper with a Self-Managed Metadata Quorum kafka/README.md at trunk · apache/kafka 帅呆了！Kafka移除了Zookeeper！ 深度解读：Kafka 放弃 ZooKeeper，消息系统兴起二次革命 Kafka Without ZooKeeper: A Sneak Peek At the Simplest Kafka Yet http://thesecretlivesofdata.com/raft/ ",
  "wordCount" : "215",
  "inLanguage": "en",
  "datePublished": "2023-07-25T02:05:51+08:00",
  "dateModified": "2023-07-25T02:05:51+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://huanglianjing.com/posts/kafka%E5%AF%B9zookeeper%E7%9A%84%E4%BE%9D%E8%B5%96%E4%B8%8E%E7%A7%BB%E9%99%A4/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "huanglianjing",
    "logo": {
      "@type": "ImageObject",
      "url": "http://huanglianjing.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://huanglianjing.com/" accesskey="h" title="huanglianjing (Alt + H)">huanglianjing</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://huanglianjing.com/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="http://huanglianjing.com/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Kafka对ZooKeeper的依赖与移除
    </h1>
    <div class="post-meta"><span title='2023-07-25 02:05:51 +0800 CST'>July 25, 2023</span>

</div>
  </header> 
  <div class="post-content"><h1 id="1-概述">1. 概述<a hidden class="anchor" aria-hidden="true" href="#1-概述">#</a></h1>
<p>Kafka自2.8开始，移除了之前用于集群的元数据管理、控制器选举等的ZooKeeper的依赖，转而使用Kraft代替，本文来聊聊这一改动的差异和影响。</p>
<p>使用过Kafka都知道，在安装Kafka之前，需要先安装Java和ZooKeeper。需要Java是因为ZooKeeper和Kafka都是用Java编写的，运行需要Java环境。而需要ZooKeeper则是因为，Kafka是使用ZooKeeper来保存集群的元数据信息和消费者信息，进行控制器选举的，因此Kafka的运行需要ZooKeeper的支持。</p>
<p>而在Kafka 2.8中，将移除对ZooKeeper的依赖，转而使用基于KRaft的Quorum控制器。</p>
<p>但是目前据官方声称有些功能还不是太完善，建议先不要用于生产环境。</p>
<h1 id="2-依赖zookeeper的kafka">2. 依赖ZooKeeper的Kafka<a hidden class="anchor" aria-hidden="true" href="#2-依赖zookeeper的kafka">#</a></h1>
<p>Kafka体系架构包含若干Producer、若干Broker、若干Consumer，以及一个ZooKeeper集群。Kafka通过ZooKeeper管理集群broker和消费者的元数据，以及用于进行控制器选举。</p>
<p><img loading="lazy" src="https://blog-1304941664.cos.ap-guangzhou.myqcloud.com/article_material/message_queue/kafka_architecture.png" alt="kafka_architecture"  />
</p>
<h1 id="3-kafka在zookeeper中的数据">3. Kafka在ZooKeeper中的数据<a hidden class="anchor" aria-hidden="true" href="#3-kafka在zookeeper中的数据">#</a></h1>
<h2 id="31-选举控制器">3.1 选举控制器<a hidden class="anchor" aria-hidden="true" href="#31-选举控制器">#</a></h2>
<p>Kafka通过ZooKeeper的临时节点实现选举控制器的功能。</p>
<p>Broker集群中，第一个启动的broker会在ZooKeeper中创建临时节点/controller，从而成为控制器。其他broker启动后也会尝试创建该临时节点，但是会收到节点已存在的异常，从而获知控制器节点已存在，并注册一个监听器。</p>
<p>在控制器关闭或断开连接后，临时节点/controller被删除，其他broker通过监听得到通知并再次尝试创建临时节点/controller，第一个创建成功的broker成为新的控制器，其他获得异常并再次进行监听。</p>
<h2 id="32-broker注册">3.2 Broker注册<a hidden class="anchor" aria-hidden="true" href="#32-broker注册">#</a></h2>
<p>Kafka的Broker是分布式部署并且相互间独立运行的，但是需要在ZooKeeper进行注册，以将整个集群的Broker服务器管理起来。</p>
<p>ZooKeeper中专门使用Broker节点进行Broker服务器列表的记录，节点路径为/brokers/ids。</p>
<p>每个Broker服务器在启动时，都会到ZooKeeper上进行注册，根据各自的Broker ID创建属于自己的节点，节点路径为/brokers/ids/[0&hellip;N]，并且将自己的IP地址和端口等信息写入该节点。</p>
<p>这个节点是一个临时节点，在Broker服务器宕机或是下线后，对应的节点就被删除了。</p>
<h2 id="33-topic注册">3.3 Topic注册<a hidden class="anchor" aria-hidden="true" href="#33-topic注册">#</a></h2>
<p>在Kafka中，一个Topic的消息会分成多个分区并分布到多个Broker上，这些分区信息以及与Broker的对应关系也都是ZooKeeper维护的，使用专门的Topic节点来记录，节点路径为/brokers/topics。</p>
<p>每一个Kafka的Topic都记录在/brokers/topics/[topic]节点中。</p>
<p>Broker服务器在启动后，会到对应的Topic节点下注册自己的Broker ID，并写入针对该Topic的提供消息存储的分区总数。如节点路径/brokers/topics/[topic]/[broker_id]的节点内容为2，表明该broker在该topic中提供了2个分区的消息存储。</p>
<p>这个节点也是一个临时节点，在Broker服务器宕机或是下线后，对应的节点就被删除了。</p>
<h2 id="34-消费者注册">3.4 消费者注册<a hidden class="anchor" aria-hidden="true" href="#34-消费者注册">#</a></h2>
<p>每个消费者在启动的时候，都会在ZooKeeper创建一个属于自己的消费者节点，且为临时节点，这样消费者出现故障或是下线后，对应的消费者节点就会被删除掉。</p>
<p>节点路径为/consumers/[group_id]/ids/[consumer_id]，节点内容为消费者订阅的Topic信息。</p>
<p>每个消费者都会对所属消费者分组的/consumers/[group_id]/ids节点进行Watcher监听，以在消费者新增或减少时，触发消费者负载均衡。同时对/brokers/ids/[0&hellip;N]的节点注册监听，在broker服务器列表发生变化时，根据情况决定是否需要消费者的负载均衡。</p>
<h2 id="35-消息分区与消费者关系">3.5 消息分区与消费者关系<a hidden class="anchor" aria-hidden="true" href="#35-消息分区与消费者关系">#</a></h2>
<p>对于每个消费者分组，Kafka都会为其分配一个全局唯一的Group ID，同时Kafka也会为每个消费者分配一个Consumer ID，通常为Hostname:UUID的形式。</p>
<p>Kafka的设计规定了每个消息分区有且只有一个消费者进行消息消费，因此需要在ZooKeeper记录消息分区与消费者之间的对应关系。节点路径/consumers/[group_id]/owners/[topic]/[broker_id-partition_id]，节点内容为Consumer ID，记录了一个消费者分组对于各个主题的分区所分配的消费者id。</p>
<h2 id="36-消息消费进度offset记录">3.6 消息消费进度Offset记录<a hidden class="anchor" aria-hidden="true" href="#36-消息消费进度offset记录">#</a></h2>
<p>ZooKeeper同时还记录了消费者对指定消息分区进行消息消费的过程中，定时提交的消费进度，即Offset。这有助于在该消费者重启，或其他消费者接管该消息分区的消息消费后，能够从之前的进度开始继续消息的消费。</p>
<p>节点路径为/consumers/[group_id]/offsets/[topic]/[broker_id-partition_id]，节点内容就是Offset值。</p>
<h1 id="4-为什么要去掉zookeeper">4. 为什么要去掉ZooKeeper<a hidden class="anchor" aria-hidden="true" href="#4-为什么要去掉zookeeper">#</a></h1>
<ol>
<li>在之前的版本中，Kafka依赖于ZooKeeper，需要管理部署两个不同的系统，让运维复杂度翻倍，还让Kafka变得沉重，进而限制了Kafka在轻量环境下的应用。去掉ZooKeeper使得Kafka的部署更简单，更轻量级。</li>
<li>ZooKeeper的分区特性限制Kafka的承载能力。在分区数量多的时候，控制器节点重新选举、分区首领切换需要进行很多的ZooKeeper操作，ZooKeeper存储的元数据数据量也更多，可能导致监听的延时增长或丢失，故障恢复耗时也更长。</li>
<li>Raft协议比ZooKeeper的ZAB协议更易懂，更高效，能提高控制器选举的速度和分区首领选举的速度。</li>
</ol>
<h1 id="5-去掉zookeeper的kafka">5. 去掉ZooKeeper的Kafka<a hidden class="anchor" aria-hidden="true" href="#5-去掉zookeeper的kafka">#</a></h1>
<h2 id="51-体系架构对比">5.1 体系架构对比<a hidden class="anchor" aria-hidden="true" href="#51-体系架构对比">#</a></h2>
<p><img loading="lazy" src="https://blog-1304941664.cos.ap-guangzhou.myqcloud.com/article_material/message_queue/kafka_without_zookeeper.png" alt="kafka_without_zookeeper"  />
</p>
<p>左图所示为目前的结构，有着3个ZooKeeper节点和4个Kafka broker节点，其中橙色的为broker节点中的控制器，控制器会向其他broker节点推送消息。</p>
<p>右图所示为去掉ZooKeeper后的结构，用控制器节点代替了ZooKeeper节点，还有普通的broker节点。控制器节点为元数据分区选举出一个橙色的leader，也称为活动控制器（active controller），其他控制器节点为follower节点。普通节点从leader拉取元数据。控制器进程与broker进程逻辑上分开，但是物理上不必分开，可以部署在同样的节点上。</p>
<p>元数据的更新也从通过向ZooKeeper注册监听的方式修改为普通节点主动从活动控制器拉取的方式。</p>
<h2 id="52-quorum控制器">5.2 quorum控制器<a hidden class="anchor" aria-hidden="true" href="#52-quorum控制器">#</a></h2>
<p>新的架构使用quorum控制器来事件来管理元数据日志，元数据日志包含集群元数据的每次更改，之前存在ZooKeeper的主题、分区、配置等信息都存在这个日志中。控制器节点中leader节点（上方右图的橙色节点）处理所有broker节点的请求，而其他follower节点（上方右图的蓝色节点节点）则复制数据，以在leader节点故障时转移状态为leader。Raft需要超过半数节点运行才能继续运行，所以三个节点的控制器允许一个节点故障，五个节点的控制器允许两个节点的故障。</p>
<p>元数据的更新也从通过向ZooKeeper注册监听的方式修改为普通节点主动从活动控制器拉取的方式。</p>
<p><img loading="lazy" src="https://blog-1304941664.cos.ap-guangzhou.myqcloud.com/article_material/message_queue/kafka_quorum_controller.png" alt="kafka_quorum_controller"  />
</p>
<h2 id="53-启动方式">5.3 启动方式<a hidden class="anchor" aria-hidden="true" href="#53-启动方式">#</a></h2>
<ol>
<li>
<p>为Kafka集群生成一个集群ID</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ ./bin/kafka-storage.sh random-uuid
</span></span><span style="display:flex;"><span>xtzWWN4bTjitpL3kfd9s5g
</span></span></code></pre></div></li>
<li>
<p>用生成的ID格式化存储目录，对于单节点在该节点运行，对多节点则在每一个节点运行</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ ./bin/kafka-storage.sh format -t &lt;uuid&gt; -c ./config/kraft/server.properties
</span></span><span style="display:flex;"><span>Formatting /tmp/kraft-combined-logs
</span></span></code></pre></div></li>
<li>
<p>在每个节点上启动Kafka，然后Kafka就启动起来了，过程中没有用到ZooKeeper</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ ./bin/kafka-server-start.sh ./config/kraft/server.properties
</span></span></code></pre></div></li>
</ol>
<h2 id="54-配置">5.4 配置<a hidden class="anchor" aria-hidden="true" href="#54-配置">#</a></h2>
<ol>
<li>
<p>每个Kafka broker需要在配置文件中配置一个process.roles，他的值可能是如下几个：</p>
<ul>
<li>broker，该节点设置为KRaft模式的一个普通节点</li>
<li>controller，该节点设置为KRaft模式的控制器节点</li>
<li>broker,controller，该节点设置为KRaft模式的既是控制器节点也是普通节点，上面说了，控制器节点和普通节点在逻辑上不同，但是是可以部署在同一个节点上的</li>
<li>未设置，该节点设置为ZooKeeper模式</li>
</ul>
</li>
<li>
<p>每个控制器节点需要配置节点id，即node.id，例如集群中配置3个控制器节点，节点id可以设置为0,1,2</p>
<pre tabindex="0"><code>node.id=0
</code></pre></li>
<li>
<p>每个控制器节点和普通节点必须配置监听地址</p>
<pre tabindex="0"><code># 普通节点
listeners=PLAINTEXT://localhost:9092
# 控制器节点
listeners=PLAINTEXT://:9093
# 普通节点&amp;控制器节点
listeners=PLAINTEXT://:9092,CONTROLLER://:9093
</code></pre></li>
<li>
<p>每个控制器节点和普通节点必须配置Quorum Voters，也就是controller.quorum.voters</p>
<pre tabindex="0"><code>controller.quorum.voters=id1@host1:port1,id2@host2:port2,id3@host3:port3
</code></pre></li>
<li>
<p>元数据日志查看的两种方式</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># kafka-dump-log</span>
</span></span><span style="display:flex;"><span>$ ./bin/kafka-dump-log.sh  --cluster-metadata-decoder --skip-record-metadata --files /tmp/kraft-combined-logs/<span style="color:#ae81ff">\@</span>metadata-0/*.log
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Metadata Shell</span>
</span></span><span style="display:flex;"><span>$ ./bin/kafka-metadata-shell.sh  --snapshot /tmp/kraft-combined-logs/<span style="color:#ae81ff">\@</span>metadata-0/00000000000000000000.log
</span></span><span style="display:flex;"><span>&gt;&gt; ls /
</span></span><span style="display:flex;"><span>brokers  local  metadataQuorum  topicIds  topics
</span></span><span style="display:flex;"><span>&gt;&gt; ls /topics
</span></span><span style="display:flex;"><span>foo
</span></span><span style="display:flex;"><span>&gt;&gt; cat /topics/foo/0/data
</span></span><span style="display:flex;"><span><span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;partitionId&#34;</span> : 0,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;topicId&#34;</span> : <span style="color:#e6db74">&#34;5zoAlv-xEh9xRANKXt1Lbg&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;replicas&#34;</span> : <span style="color:#f92672">[</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">]</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;isr&#34;</span> : <span style="color:#f92672">[</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">]</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;removingReplicas&#34;</span> : null,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;addingReplicas&#34;</span> : null,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;leader&#34;</span> : 1,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;leaderEpoch&#34;</span> : 0,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;partitionEpoch&#34;</span> : <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>&gt;&gt; exit
</span></span></code></pre></div></li>
</ol>
<h2 id="55-raft算法">5.5 Raft算法<a hidden class="anchor" aria-hidden="true" href="#55-raft算法">#</a></h2>
<p>以下动画网页很好地展示了Raft算法的原理和同步、复制、领导选举等步骤：</p>
<p><a href="http://thesecretlivesofdata.com/raft/">http://thesecretlivesofdata.com/raft/</a></p>
<h2 id="56-性能对比">5.6 性能对比<a hidden class="anchor" aria-hidden="true" href="#56-性能对比">#</a></h2>
<p>quorum控制器的引入极大降低了多分区情形下关闭与重启的耗时，从而提高了分区的数量上限。</p>
<p>以下为Kafka在新旧的架构中拥有200万分区时关闭和启动恢复的速度，可以看出在新的架构下时间是大大缩短的。</p>
<p><img loading="lazy" src="https://blog-1304941664.cos.ap-guangzhou.myqcloud.com/article_material/message_queue/kafka_shutdown_2m_partitions_time.png" alt="kafka_shutdown_2m_partitions_time"  />
</p>
<h1 id="参考">参考<a hidden class="anchor" aria-hidden="true" href="#参考">#</a></h1>
<ul>
<li><a href="https://book.douban.com/subject/26292004/">《从Paxos到Zookeeper》</a></li>
<li><a href="https://book.douban.com/subject/27665114/">《Kafka权威指南》</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum">KIP-500: Replace ZooKeeper with a Self-Managed Metadata Quorum</a></li>
<li><a href="https://github.com/apache/kafka/blob/trunk/config/kraft/README.md">kafka/README.md at trunk · apache/kafka</a></li>
<li><a href="https://blog.csdn.net/lycyingO/article/details/116246371">帅呆了！Kafka移除了Zookeeper！</a></li>
<li><a href="https://www.infoq.cn/article/PHF3gFjUTDhWmctg6kXe">深度解读：Kafka 放弃 ZooKeeper，消息系统兴起二次革命</a></li>
<li><a href="https://www.confluent.io/blog/kafka-without-zookeeper-a-sneak-peek/">Kafka Without ZooKeeper: A Sneak Peek At the Simplest Kafka Yet</a></li>
<li><a href="http://thesecretlivesofdata.com/raft/">http://thesecretlivesofdata.com/raft/</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://huanglianjing.com/tags/kafka/">Kafka</a></li>
      <li><a href="http://huanglianjing.com/tags/zookeeper/">ZooKeeper</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="http://huanglianjing.com/">huanglianjing</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
